{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4xZopwvoS7E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ckZrdsFobJA"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install datasets\n",
        "!pip install sentencepiece\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u0IX6V_odVh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CgDh03Wodg_"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PozZ3Axod1m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVvDh3QTof4_"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = './drive/MyDrive/ML/Nuage/jigsaw/data'\n",
        "MODEL_PATH = './drive/MyDrive/ML/Nuage/jigsaw/models'\n",
        "os.listdir(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybI3bQ0opZS_"
      },
      "outputs": [],
      "source": [
        "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "\n",
        "df1 = pd.read_csv(f'{DATA_PATH}/jigsaw-toxic-comment-train.csv').rename(columns={\"comment_text\": \"text\"}).sample(frac=1, random_state=42)\n",
        "df1['labels']= df1[label_cols].values.astype(float).tolist()\n",
        "\n",
        "df2 = pd.read_csv(f'{DATA_PATH}/jigsaw-unintended-bias-train.csv').rename(columns={\n",
        "    \"comment_text\": \"text\",\n",
        "    \"severe_toxicity\":\"severe_toxic\",\n",
        "    \"identity_attack\":\"identity_hate\"\n",
        "}).sample(frac=1, random_state=42)\n",
        "for col in label_cols:\n",
        "  df2[col] = (df2[col] > 0.5).astype(float)\n",
        "df2['labels']= df2[label_cols].values.tolist()\n",
        "\n",
        "# Sampling and merge\n",
        "df=pd.concat((df1,df2))\n",
        "df1=df[\n",
        "  (df.toxic.astype(int)==1)  |\n",
        "  (df.severe_toxic.astype(int)==1) |\n",
        "  (df.obscene.astype(int)==1)  |\n",
        "  (df.threat.astype(int)==1)  |\n",
        "  (df.insult.astype(int)==1)  |\n",
        "  (df.identity_hate.astype(int)==1)\n",
        "]\n",
        "df2=df[(df.toxic==0)&(df.obscene==0)&(df.threat==0)&(df.insult==0)&(df.identity_hate==0)][:len(df1)//2]\n",
        "df=pd.concat((df1,df2))[[\"text\",\"labels\"]]\n",
        "\n",
        "#df = pd.concat((df1[[\"text\",\"labels\"]],df2[[\"text\",\"labels\"]]))\n",
        "#del df1,df2\n",
        "#df=df1.copy()\n",
        "#del df1\n",
        "\n",
        "print(f\"Total: {len(df):,}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbgJoOBComDt"
      },
      "outputs": [],
      "source": [
        "id2label = dict([(i,label) for i, label in enumerate(label_cols)])\n",
        "label2id = dict([(label,i) for i, label in enumerate(label_cols)])\n",
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBrwx2CPomFt"
      },
      "outputs": [],
      "source": [
        "split_index = int(len(df)*0.95)\n",
        "train_dataset = Dataset.from_pandas(df[[\"text\",\"labels\"]][:split_index])\n",
        "test_dataset = Dataset.from_pandas(df[[\"text\",\"labels\"]][split_index:])\n",
        "\n",
        "train_dataset = train_dataset.train_test_split(test_size=0.05)\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB6zjjB8onin"
      },
      "outputs": [],
      "source": [
        "ROBERTA_MODEL = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(ROBERTA_MODEL, num_labels=len(label_cols), problem_type=\"multi_label_classification\", id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So8lH1G9oqSw"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOqMl4u0osqC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "import numpy as np\n",
        "    \n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    try:\n",
        "      roc_auc_micro = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "      roc_auc_macro = roc_auc_score(y_true, y_pred, average = 'macro')\n",
        "    except:\n",
        "      roc_auc_micro=0\n",
        "      roc_auc_macro=0\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
        "    # return as dictionary\n",
        "    metrics = {'f1_macro': f1_macro_average,\n",
        "               'f1_micro': f1_micro_average,\n",
        "               'roc_auc_micro': roc_auc_micro,\n",
        "               'roc_auc_macro': roc_auc_macro,\n",
        "               'accuracy': accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, \n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds, \n",
        "        labels=p.label_ids)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUCxq44eouIQ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./checkpoints',          # output directory\n",
        "    num_train_epochs=10,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=2000,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    #eval_steps=10000,\n",
        "    #save_steps=10000,\n",
        "    learning_rate=2e-5,\n",
        ")\n",
        "training_args.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcDs2hy_o3rS"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "def calculate_pos_weights(class_counts):\n",
        "  pos_weights = np.ones_like(class_counts)\n",
        "  neg_counts = [len(y_train)-pos_count for pos_count in class_counts]\n",
        "  for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n",
        "    pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n",
        "\n",
        "  return torch.as_tensor(pos_weights, dtype=torch.float)\n",
        "\n",
        "y_train = np.array(df[[\"text\",\"labels\"]][:split_index]['labels'].to_list())\n",
        "weights = calculate_pos_weights(y_train.sum(axis=0)).to('cuda')\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ROOYFb5o7t0"
      },
      "outputs": [],
      "source": [
        "custom_loss = BCEWithLogitsLoss(pos_weight=weights)\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "      target = inputs.get(\"labels\")\n",
        "      outputs = model(**inputs)\n",
        "      logits = outputs.get(\"logits\")\n",
        "\n",
        "      loss = custom_loss(logits, target)\n",
        "      return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHmxKnR2o-_O"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset[\"train\"],\n",
        "    eval_dataset=train_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfBj2YR2pA1B"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(MODEL_PATH+\"/trained_v1\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (main, Sep  6 2022, 13:54:47) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "761dcc2e730670a5406af1e4d952870e7bed611eb3687403cb5a8f4459e63789"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
