{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb698957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b440f3",
   "metadata": {},
   "source": [
    "# 1. MLMA Hate Speech Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8929",
   "metadata": {},
   "source": [
    "- Paper: https://aclanthology.org/D19-1474.pdf\n",
    "- Dataset Link: https://huggingface.co/datasets/nedjmaou/MLMA_hate_speech\n",
    "\n",
    "------\n",
    "\n",
    "**Directness label:** the explicitness of the tweet either `direct` or `indirect`. This should be based on whether the target is explicitly named, or less easily discernible, especially if the tweet contains humor, metaphor, or figurative speech. \n",
    "\n",
    "**Hostility type** (multilabel) To identify the hostility type of the tweet, we stick to the following conventions:\n",
    "- (1) if the tweet sounds dangerous, it should be labeled as `abusive` \n",
    "- (2) according to the degree to which it spreads hate and the tone its author uses, it can be `hateful`, `offensive` or - `disrespectful`\n",
    "- (3) if the tweet expresses or spreads fear out of ignorance against a group of individuals, it should be labeled as `fearful` \n",
    "- (4) otherwise it should be annotated as `normal`. \n",
    "\n",
    "**Target** whether the tweet insults or discriminates against people based on their (1) `origin`, (2) `religious affiliation`, (3) `gender`, (4) `sexual orientation`, (5) `special needs` or (6) `other`\n",
    "\n",
    "**Target group** We determined 16 common target groups tagged by the annotators after the first annotation step. The annotators had to decide on whether the tweet is aimed at women, people of `African descent`, `Hispanics`, `gay people`, `Asians`, `Arabs`, `immigrants in general`, `refugees`; people of different religious affiliations such as `Hindu`, `Christian`, `Jewish` people, and `Muslims`; or from political ideologies `socialists`, and others. We also provided the annotators with a category to cover hate directed towards one `individual`, which cannot be generalized. In case the tweet targets morethan one group of people, the annotators should choose the group which would be the most affected by it according to them. \n",
    "\n",
    "**Sentiment of the annotator** We claim that the choice of a suitable emotion representation model is key to this sub-task, given the subjective nature and social ground of the annotator’s sentiment analysis. After collecting the annotation results of the pilot dataset regarding how people feel about the tweets, and observing the added categories, we adopted a range of sentiments that are in the negative and neutral scales of the hourglass of emotions\n",
    "introduced by Cambria et al. (2011). This model includes sentiments that are connected to objectively assessed natural language opinions, and excludes what is known as self-conscious or moral emotions such as shame and guilt. Our labels include shock, sadness, disgust, anger, fear, confusion in case of ambivalence, and indifference. This is the second multilabel task of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3db114",
   "metadata": {},
   "source": [
    "## 1.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nedjmaou/MLMA_hate_speech\")\n",
    "pd_dataset = dataset['train'].to_pandas()\n",
    "pd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Arabic total: {len(pd_dataset[:3353])}\")\n",
    "print(f\"English total: {len(pd_dataset[3353:14647])}\")\n",
    "print(f\"French total: {len(pd_dataset[14647:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d92dbe",
   "metadata": {},
   "source": [
    "## 1.2. Label Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01822a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd_dataset.sentiment.apply(lambda x: x.split('_')).to_list()\n",
    "sentiments_unique = {x for l in sentiments for x in l}\n",
    "\n",
    "print(f\"Unique `Sentiments`\\n\\n{sentiments_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique `Directness`\\n\\n{list(pd_dataset.directness.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique `Target`\\n\\n{list(pd_dataset.target.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ecc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique `Group`\\n\\n{list(pd_dataset.group.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_sentiments = pd_dataset.annotator_sentiment.apply(lambda x: x.split('_')).to_list()\n",
    "annotator_sentiments_unique = {x for l in annotator_sentiments for x in l}\n",
    "\n",
    "print(f\"Unique `Annotator Sentiments`\\n\\n{annotator_sentiments_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1a801",
   "metadata": {},
   "source": [
    "## 1.3. Comments\n",
    "Might be useful for us and it is multi-lingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e59e3c",
   "metadata": {},
   "source": [
    "# 2. ~CyberAgressionAdo-v1~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d277d8b",
   "metadata": {},
   "source": [
    "- Paper: https://hal.archives-ouvertes.fr/hal-03765860/document\n",
    "- Dataset Link: https://github.com/aollagnier/CyberAgressionAdo-v1\n",
    "---\n",
    "\n",
    "Participant role. Values (5): `victim, bully, victim_support, bully_support, mediator, conciliator`\n",
    "\n",
    "Hate-speech: Values (2): `yes, no`\n",
    "\n",
    "Type of verbal abuse. Values (4): `blaming, name_calling, threat, denigradation, other-aggression`\n",
    "\n",
    "Target. Values (5): `victim, bully, victim_support, bully_support, mediator, conciliator`\n",
    "\n",
    "Humor. Values (2): `yes, no`\n",
    "\n",
    "## 2.1. Comments\n",
    "\n",
    "After a small investigation it doesn't seems promising. Poorly labelled files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1104d9e",
   "metadata": {},
   "source": [
    "# 3. HateSpeechMulti\n",
    "\n",
    "https://www.kaggle.com/datasets/ludovick/hatespeechmulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/cleaned_data_hatespeech.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Language distribution\")\n",
    "df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Toxic distribution of French Dataset\")\n",
    "df[df.lang=='fr'].toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f05c56",
   "metadata": {},
   "source": [
    "# 4. CONAN\n",
    "\n",
    "https://github.com/marcoguerini/CONAN\n",
    "\n",
    "---\n",
    "CONAN is a multilingual and expert-based dataset of hate speech/counter-narrative pairs in English, French and Italian, focused on Islamophobia.\n",
    "\n",
    "**Dataset description**\n",
    "The dataset consists of 4078 pairs over the 3 languages. Together with the data we also provide 3 types of metadata: expert demographics, hate speech sub-topic and counter-narrative type. The dataset is augmented through translation (from Italian/French to English) and paraphrasing, which brought the total number of pairs to 14.988.\n",
    "\n",
    "(*)The original number was 15.024 but after post-hoc analysis, we deleted 9 original pairs (36 pairs including augmented ones) because they did not meet the required standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cfe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/CONAN.csv')\n",
    "df['lang'] = df.cn_id.apply(lambda x: x[:2])\n",
    "df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36136d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr = df[df.lang == 'FR']\n",
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77151523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.hsType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.hsSubType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fc70d",
   "metadata": {},
   "source": [
    "# 5. CyberBullying Classification (EN)\n",
    "\n",
    "https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification\n",
    "\n",
    "---\n",
    "In light of all of this, this dataset contains more than 47000 tweets labelled according to the class of cyberbullying:\n",
    "\n",
    "* Age;\n",
    "* Ethnicity;\n",
    "* Gender;\n",
    "* Religion;\n",
    "* Other type of cyberbullying;\n",
    "* Not cyberbullying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/cyberbullying_tweets.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47570bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cyberbullying_type=='gender'].tweet_text.to_list()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e52f1",
   "metadata": {},
   "source": [
    "# 6. Jigsaw (EN)\n",
    "\n",
    "The primary data for the competition is, in each provided file, the comment_text column. This contains the text of a comment which has been classified as toxic or non-toxic (0…1 in the toxic column). The train set’s comments are entirely in english and come either from Civil Comments or Wikipedia talk page edits. The test data's comment_text columns are composed of multiple non-English languages.\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "761dcc2e730670a5406af1e4d952870e7bed611eb3687403cb5a8f4459e63789"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
