{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>pred_openai</th>\n",
       "      <th>pred_hive</th>\n",
       "      <th>custom</th>\n",
       "      <th>pred_openai_binary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump est un bouffon traître.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[insult]</td>\n",
       "      <td>[toxic]</td>\n",
       "      <td>[toxic, insult]</td>\n",
       "      <td>No</td>\n",
       "      <td>[toxic, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combien de temps encore notre pays peut-il per...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[insult, identity_attack]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[toxic, obscene]</td>\n",
       "      <td>No.</td>\n",
       "      <td>[toxic, obscene, insult]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LMAO. Mexicains blancs contre Mexicains bruns....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[insult]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[toxic, identity_attack]</td>\n",
       "      <td>No</td>\n",
       "      <td>[toxic, identity_attack]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  obscene  threat  \\\n",
       "0                      Trump est un bouffon traître.      1        0       0   \n",
       "1  Combien de temps encore notre pays peut-il per...      1        1       0   \n",
       "2  LMAO. Mexicains blancs contre Mexicains bruns....      1        0       0   \n",
       "\n",
       "   insult  identity_attack                pred_openai pred_hive  \\\n",
       "0       1                0                   [insult]   [toxic]   \n",
       "1       1                0  [insult, identity_attack]        []   \n",
       "2       0                1                   [insult]        []   \n",
       "\n",
       "                     custom pred_openai_binary                     label  \n",
       "0           [toxic, insult]                 No           [toxic, insult]  \n",
       "1          [toxic, obscene]                No.  [toxic, obscene, insult]  \n",
       "2  [toxic, identity_attack]                 No  [toxic, identity_attack]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./prediction_results.csv')\n",
    "\n",
    "label_map = ['toxic','obscene','threat','insult','identity_attack']\n",
    "hive_map = {\n",
    "    \"sexual\":[\"obscene\",\"insult\"],\n",
    "    \"hate\":[\"identity_attack\"], \n",
    "    \"violence\":[\"threat\"], \n",
    "    \"bullying\":[\"toxic\"]\n",
    "}\n",
    "preds_openai=[]\n",
    "preds_hive=[]\n",
    "for i,row in df.iterrows():\n",
    "    #Parse Hive\n",
    "    if type(row.pred_hive) != str:\n",
    "        pred_hive = []\n",
    "    else:\n",
    "        pred_hive = [item['class'] for item in json.loads(row.pred_hive.replace(\"'\",'\"')) if item['score'] > 0]\n",
    "        pred_hive = [hive_map[pred] for pred in pred_hive if pred in hive_map]\n",
    "        #flatten list of lists\n",
    "        pred_hive = [item for sublist in pred_hive for item in sublist]\n",
    "    preds_hive.append(pred_hive)\n",
    "\n",
    "    # Parse OpenAI\n",
    "    if row.pred_openai is None:\n",
    "        pred_openai = []\n",
    "    else:\n",
    "        pred_openai=[]\n",
    "        for item in row.pred_openai.split(','):\n",
    "            if item.strip().isnumeric() and int(item) >= 1 and int(item) <= 5:\n",
    "                pred_openai.append(label_map[int(item)-1])\n",
    "            elif item.lower() in  label_map:\n",
    "                pred_openai.append(item.lower())\n",
    "            elif \"no categories\" in item.lower() or \"none\" in item.lower() or item==\"0\":\n",
    "                item = None\n",
    "            elif \".\" in item:\n",
    "                pred_openai.append(label_map[int(item.split(\".\")[0])-1])\n",
    "            else:\n",
    "                print(\"Undefined prediction: \",item)\n",
    "\n",
    "    preds_openai.append(pred_openai)\n",
    "\n",
    "\n",
    "df['pred_openai'] = preds_openai             \n",
    "df['pred_hive'] = preds_hive         \n",
    "df['custom'] = df['custom'].apply(lambda x:json.loads(x.replace(\"'\",'\"')))                 \n",
    "df['custom'] = df['custom'].apply(lambda x:[item.replace(\"identity_hate\",\"identity_attack\").replace(\"severe_toxic\",\"toxic\") for item in x])                 \n",
    "df['custom'] = df['custom'].apply(lambda x:list(set(x)))                 \n",
    "    \n",
    "df['label'] = df.apply(lambda x: [\n",
    "        label\n",
    "        for label in label_map\n",
    "        if int(x[label]) == 1\n",
    "    ], axis=1)\n",
    "df.head(3)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 .Multilabel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform([label_map])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "identity_attack       0.77      0.48      0.59       139\n",
      "         insult       0.24      0.07      0.10       287\n",
      "        obscene       0.16      0.10      0.13       128\n",
      "         threat       0.49      0.67      0.56       100\n",
      "          toxic       0.89      0.27      0.42       396\n",
      "\n",
      "      micro avg       0.54      0.26      0.35      1050\n",
      "      macro avg       0.51      0.32      0.36      1050\n",
      "   weighted avg       0.57      0.26      0.33      1050\n",
      "    samples avg       0.19      0.16      0.16      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        mlb.transform(df.label.to_list()),\n",
    "        mlb.transform(df.pred_hive.to_list()), \n",
    "        target_names=mlb.classes_,\n",
    "        zero_division=0\n",
    "        )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "identity_attack       0.22      0.53      0.31       139\n",
      "         insult       0.37      0.83      0.51       287\n",
      "        obscene       0.22      0.03      0.05       128\n",
      "         threat       0.60      0.06      0.11       100\n",
      "          toxic       0.56      0.02      0.04       396\n",
      "\n",
      "      micro avg       0.32      0.32      0.32      1050\n",
      "      macro avg       0.39      0.30      0.21      1050\n",
      "   weighted avg       0.43      0.32      0.21      1050\n",
      "    samples avg       0.21      0.20      0.19      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        mlb.transform(df.label.to_list()),\n",
    "        mlb.transform(df.pred_openai.to_list()), \n",
    "        target_names=mlb.classes_,\n",
    "        zero_division=0\n",
    "        )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "identity_attack       0.92      0.96      0.94       139\n",
      "         insult       0.80      0.91      0.85       287\n",
      "        obscene       0.90      0.89      0.89       128\n",
      "         threat       0.99      0.91      0.95       100\n",
      "          toxic       0.59      0.96      0.73       396\n",
      "\n",
      "      micro avg       0.73      0.93      0.82      1050\n",
      "      macro avg       0.84      0.93      0.87      1050\n",
      "   weighted avg       0.77      0.93      0.83      1050\n",
      "    samples avg       0.49      0.60      0.52      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        mlb.transform(df.label.to_list()),\n",
    "        mlb.transform(df.custom.to_list()), \n",
    "        target_names=mlb.classes_,\n",
    "        zero_division=0\n",
    "        )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.48      0.93      0.64       350\n",
      "       Toxic       0.93      0.46      0.61       650\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.70      0.70      0.62      1000\n",
      "weighted avg       0.77      0.62      0.62      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_binary = df.pred_hive.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "true_binary = df.label.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        true_binary,\n",
    "        pred_binary, \n",
    "        target_names=[\"Normal\",\"Toxic\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Multi-Label to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.55      0.54      0.54       350\n",
      "       Toxic       0.75      0.76      0.76       650\n",
      "\n",
      "    accuracy                           0.68      1000\n",
      "   macro avg       0.65      0.65      0.65      1000\n",
      "weighted avg       0.68      0.68      0.68      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_binary = df.pred_openai.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "true_binary = df.label.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        true_binary,\n",
    "        pred_binary, \n",
    "        target_names=[\"Normal\",\"Toxic\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking for Binary Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.39      0.99      0.56       350\n",
      "       Toxic       0.96      0.16      0.27       650\n",
      "\n",
      "    accuracy                           0.45      1000\n",
      "   macro avg       0.68      0.57      0.42      1000\n",
      "weighted avg       0.76      0.45      0.37      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_binary = df.pred_openai_binary.apply(lambda x: \"yes\" in x.lower()).astype(int).to_list()\n",
    "true_binary = df.label.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        true_binary,\n",
    "        pred_binary, \n",
    "        target_names=[\"Normal\",\"Toxic\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.92      0.86      0.89       350\n",
      "       Toxic       0.93      0.96      0.94       650\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.91      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_binary = df.custom.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "true_binary = df.label.apply(lambda x: len(x) > 0).astype(int).to_list()\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        true_binary,\n",
    "        pred_binary, \n",
    "        target_names=[\"Normal\",\"Toxic\"]\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "761dcc2e730670a5406af1e4d952870e7bed611eb3687403cb5a8f4459e63789"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
